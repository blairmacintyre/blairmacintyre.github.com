---
id: 430
title: 'It all started with Snow Crash &#8230;'
date: 2012-04-18T10:41:22+00:00
author: blair
layout: post
guid: http://ael.gatech.edu/blair/?p=196
permalink: /2012/04/18/it-all-started-with-snow-crash/
---
Ok, I know I&#8217;m either going to love or hate a blog post about an internal research project on mobile AR, when the first line is

> It all started with _Snow Crash_.

Michael Abrash, game and tech industry veteran, and now at Valve, recently wrote [a blog post](http://blogs.valvesoftware.com/abrash/valve-how-i-got-here-what-its-like-and-what-im-doing-2/) about his journey to Valve and the current internal research project on what he calls &#8220;wearable computing.&#8221;   For the rest of us, though, the description of what they are doing sounds exactly what researchers like me would call fully immersive AR. (Ironically, the folks at Google are doing something that the research community would traditionally call wearable computing, and folks want to call it AR, but that&#8217;s a different conversation.)

Suffice to say, I loved this post.  Now, he doesn&#8217;t really provide much information on what they are doing, so it&#8217;s not that it&#8217;s the details that are what&#8217;s compelling.  But, I find myself really excited about the idea of a top tier game company doing an internal project that is focused on what is arguably the distant future of AR gaming.

Sure, companies like Sony and Nintendo and Microsoft have built AR games for recently, and the core feature of the PS Vita, 3DS and Kinect is the ability to do augmented reality style games.  But, all of these products and projects are focused on the near future, on games and experiences that will be possible with this hardware. In my research group, especially our [Augmented Reality Game Studio](http://argamestudio.org), we&#8217;ve been working on similar kinds of games for many years, and so I obviously see the value in trying to figure out how to create compelling AR game experiences with current hardware and software (heck, we will hopefully be releasing one of our games, [Nerdherder](http://www.micronerd.net/), into the app stores soon!)

But, lately I&#8217;ve been thinking that it&#8217;s time to refocus on the future.  When we started the AR Game Studio, it still wasn&#8217;t easy to build AR games on mobile devices, and we were lucky to team up with Qualcomm to see what would be possible with what has since become [Vuforia](http://www.qualcomm.com/solutions/augmented-reality), their free AR SDK (that is probably one of the best solutions for doing AR on mobiles right now).  Now that Vuforia is out there, and is available for the Unity game engine as a free plugin, everyone can get in on the action!  If you can build games in Unity, you can build AR games!    I will keep teaching AR game design classes, and working with students in my lab to build these games.  And my students and I will keep investigating how to create compelling experiences of this sort (with the advantage that we can study what others make, too, not just what we&#8217;ve built).

But, my first love is working with technology and concepts on the cutting edge that are (for all intents and purposes) impossible on a large scale, rather than studying and understanding what people can do right now.

Which brings me back to Michael Abrash.  I have no idea what they are really doing.  But, the thing I&#8217;ve learned about game design over the past half dozen years, building games in my lab and at [my company](http://www.aurainteractive.com), is that game design is different than research, even if your research needs you to build a game.  Unlike researchers or product engineers, game designers are trained and focused on creating compelling experiences.  Things only matter insofar as they are perceived by the player and impact the experience of the player.  If the experience sucks, the experience sucks:  there is no &#8220;oh, it only works if you do this&#8221; or &#8220;imagine what it would be like if this was better or that was better&#8221;.  There is no making it work well enough for the paper, or the video, or the evaluation.  There is only the experience.   As Miyagi said in _[The Karate Kid](http://tvtropes.org/pmwiki/pmwiki.php/Main/TheKarateKid "http://tvtropes.org/pmwiki/pmwiki.php/Main/TheKarateKid")_:

> Miyagi: Now, ready?
  
> Daniel: Yeah, I guess so.
  
> Miyagi: Daniel-san, must talk. Walk on road, hm? Walk left side, safe. Walk right side, safe. Walk middle, sooner or later, _(makes squish gesture)_ get squish just like grape. Here, karate, same thing. Either you karate do &#8220;yes&#8221;, or karate do &#8220;no&#8221;. You karate do &#8220;guess so&#8221;, _(makes squish gesture)_ just like grape. Understand?

AR (or wearable computing, as Michael calls it) will not really be possible any time soon.  To safely and unobtrusively integrate content with your view of the world around you, you must be able to achieve the kind of integration we see in live sports on TV, and that simply isn&#8217;t going to be feasible soon.  But if you want to work towards that dream, you need to decide:  am I building something to take me one step in that direction (e.g., my games in the AR Game Studio, or release of the Argon AR-enabled web browser, Google&#8217;s Project Glass) or are we going to just try to create a compelling experience of what it might be like when we get there.

I don&#8217;t know if the later is what Valve really wants to do, but I think a game studio that is saying up front

> To be clear, this is R&D – it doesn’t in any way involve a product at this point, and won’t for a long while, if ever – so please, no rumors about Steam glasses being announced at E3.

is likely to do something interesting and exciting and different than the other projects you see around the web.

Too bad Valve isn&#8217;t in Atlanta.